<p align="center">
  <img src="figures/MEDAKA_logo.png" width="150" alt="Medaka Logo" />
</p>
<h1 align="center">MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models</h1>

<div align="center">
  <a href="" download>
    <img src="https://img.shields.io/badge/DOWNLOAD%20MEDAKA%20-CSV-blue?style=for-the-badge&logo=csv&logoColor=white" alt="Download MEDAKA in CSV" style="height: 42px;">
  </a>
</div>


---

## Table of Contents

| [TL;DR](#tldr) | [Features](#features) | [Pipeline](#pipeline) | [Description](#description) | [Installation](#installation) | [Usage](#usage) |

---

<a name="tldr"></a>
## ‚ö° TL;DR [[Back to Top]Ô∏é](#table-of-contents)

We introduce a two-fold contribution:

1Ô∏è‚É£ A **hackable, end-to-end pipeline** for automatically constructing KGs from unstructured text using a **web scraper and an LLM**.  
2Ô∏è‚É£ A **curated dataset** generated by applying this method to publicly available **drug leaflets**, capturing drug-centric attributes often missing in existing biomedical KGs.

**[Download MEDAKA]()** to explore clinically relevant information beyond molecular data.

### <a name="figure-1-pipeline"></a>
<p align="center">
  <img src="figures/Medaka_Final_Pipeline_v1.png" alt="Pipeline Overview" width="100%">
</p>
<em>Figure 1: Pipeline Overview - Drug leaflets are scraped and parsed into text, queried five times with an LLM for entity‚Äìrelation extraction, filtered via majority voting, and assembled into the final KG.</em>

---

<a name="features"></a>
## ‚ú® Features [[Back to Top]Ô∏é](#table-of-contents)

- **End-to-End KG Pipeline:**  A modular and hackable pipeline that extracts structured knowledge from unstructured text using a web scraper and an LLMs. 

- **Drug Leaflet-Centric KG:**  *MEDAKA* is constructed from 13,000+ drug leaflets sourced from the HPRA website, capturing real-world, clinical data important for both patients and medical practitioners.

- **LLM-Based Information Extraction with Majority Voting:** Utilizes **LLaMA 3.3 70B Instruct** model for prompt-based extraction of subject‚Äìrelation‚Äìobject triples directly from full-text PDFs. Each leaflet is queried multiple times, and triples occurring at least 3 times are retained, boosting precision and reducing LLM hallucination.

- **Ready-to-Use Dataset:** *MEDAKA* can be downloaded in a ready-to-use CSV format.

- **Broader Clinical Coverage:**  Captures practical features often missing in other biomedical KGs and databases, including storage information, physical appearance (shape, color), and inactive ingredients.

- **Generalizable & Open Source:**  The pipeline is reusable and adaptable across domains and document types, and the full codebase is open source.

---

<a name="pipeline"></a>
## üß¨ Pipeline [[Back to Top]Ô∏é](#table-of-contents)

We present an end-to-end pipeline for constructing a biomedical KG from unstructured drug leaflet data.

- The process begins with **data collection**, where drug leaflets are scraped from online pharmacies using a Python-based web scraper and converted into machine-readable text.

- The raw text is then processed using a **prompt-based approach with an LLM**, queried multiple times to extract drug-related entities and relationships as subject‚Äìrelation‚Äìobject triplets.

- A **majority voting step** retains only triplets occurring three or more times across generations, ensuring consistency and reliability.

- The validated triplets are transformed into **graph nodes and labeled edges**, forming the MEDAKA KG.

Finally, the structured data is exported in **CSV format**, forming the foundation of the MEDAKA KG.

> üìå *[Figure 1](#figure-1-pipeline) above shows a visual overview of the pipeline, highlighting each step from raw data collection to graph construction.*
---

<a name="description"></a>
## üìä Description [[Back to Top]Ô∏é](#table-of-contents)

As a demonstration of our general pipeline for the construction of KGs, we implemented and evaluated a case study focused on drug leaflets. The resulting KG, MEDAKA consists of 41,142 nodes and 466,359 directed, labeled edges. The following *[Figure 2](#figure-2)* and *[Figure 3](#figure-3)* show the distribution of various entity types and relation types in the dataset.

<a name="figure-2"></a>
<a name="figure-3"></a>
<table width="100%">
  <tr>
    <td align="center" width="50%">
      <img src="figures/Medaka_Entity_Distribution.png" alt="Figure 1" width="100%"/><br>
      <em>Figure 2: Entity Type Distribution</em>
    </td>
    <td align="center" width="50%">
      <img src="figures/Medaka_Edge_Distribution.png" alt="Figure 2" width="100%"/><br>
      <em>Figure 3: Relation Type Distribution</em>
    </td>
  </tr>
</table>

---

<a name="installation"></a>
## Installation [[Back to Top]Ô∏é](#table-of-contents)

Follow these steps to set up the project locally:

### 1. Clone the repository
```bash
git clone https://github.com/medaka25/medaka.git
cd medaka
```

### 2. Create and activate virtual environment
Choose one of the following:

- **Option A: Using `venv`**
```bash
python3 -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```
- **Option B: Using `conda`**
```bash
conda create --name medaka-env python=3.10
conda activate medaka-env
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

---

<a name="usage"></a>
## Usage [[Back to Top]Ô∏é](#table-of-contents)
The pipeline is divided into multiple stages for modular processing:

### 1. Data Scraping
- **Step 1: Extract Source Code**
```bash
python scripts/scrape_html_sources.py
```
Scrapes the [HPRA website](https://www.hpra.ie) using BeautifulSoup and extracts source code with links to drug leaflet pages. You may use a scraper of your choice.
- **Step 2: Download PDFs from Extracted URLs**
```bash
python scripts/download_pdfs.py --rtf_path ./sourcecode.rtf --output_folder ./data
```
Downloads the actual drug leaflet PDFs based on the extracted page source code.

### 2. Information Extraction Using LLM
- **Step 3: Extract Structured Triples from PDFs**
```bash
python scripts/extract_information.py \
  --pdf-dir ./data \
  --output-csv ./medaka.csv \
  --processed-log ./processed_files.txt \
  --cache-dir ./cache \
  --api-url <your_llm_url> \
  --model-name <your_model_name> \
  --api-key <your_api_key>
```
Uses an LLM (e.g., LLaMA 3 70B Instruct) to extract drug-related information using prompts in the form of  structured biomedical triples (drug name, relation, object). It applies majority voting across multiple generations to improve reliability, assigns confidence scores, converts triples to lowercase, removes duplicates and writes results into a CSV file. Replace the LLM arguments with your preferred model and endpoint.


### 3. KG Construction
- **Step 4: Filter triples based on confidence scores**
```bash
python scripts/filter_triples.py \
  --input-csv ./medaka.csv \
  --output-csv ./medaka_filtered.csv \
  --threshold 0.5
```
Refines the extracted triples by filtering them based on the confidence score assigned during majority voting. Only triples with Confidence greater than or equal to the specified threshold (default: 0.5) are kept. The result is a cleaned CSV containing high-confidence triples, forming the KG, Medaka.

---

### Graph Statistics
```bash
python scripts/medaka_stats.py --input-csv ./medaka_filtered.csv
```
Explores key graph statistics of MEDAKA including:
- Node & edge counts
- Predicate-wise statistics
- Degree distributions, graph connectivity, betweenness centrality and assortativity
- Top drugs based on the count of relations

---

### LLM-as-Judge Evaluation
```bash
python scripts/llm_eval.py \
  --input-txt ./Medaka_Validation_Triples.txt \
  --pdf-dir ./data \
  --out-csv ./Medaka_Triples_Validated.csv \
  --pdf-suffix .pdf \
  --temperature <enter_temperature> \
  --max-tokens <enter_max_tokens> \
  --max-triples-per-call <enter_max_triples_per_call> \
  --max-leaflet-chars <enter_max_leaflet_chars> \
  --api-url  <your_llm_url> \
  --model-name <your_model_name> \
  --api-key <your_api_key>
```
This step is part of the evaluation process which validates a set of random triples against the original leaflet PDFs using an LLM-as-Judge framework. Each triple is labeled as Correct, Incorrect, or Partially Correct, with a short reasoning grounded in the leaflet text. Results are saved to a CSV file.
